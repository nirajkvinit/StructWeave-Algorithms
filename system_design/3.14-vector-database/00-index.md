# Vector Database

[← Back to System Design Index](../README.md)

## Overview

A **Vector Database** is a specialized database designed to store, index, and query high-dimensional vectors (embeddings) generated by machine learning models. Unlike traditional databases that use exact matching, vector databases find items by **similarity** using distance metrics like cosine similarity or Euclidean distance. They power modern AI applications including semantic search, recommendation systems, RAG (Retrieval-Augmented Generation) for LLMs, image similarity, and anomaly detection.

---

## Complexity Rating

| Aspect | Rating | Justification |
|--------|--------|---------------|
| **Overall** | **High** | Combines distributed systems complexity with specialized ANN algorithms |
| Algorithm Complexity | Very High | HNSW, IVF, Product Quantization require deep understanding |
| Scale Challenges | High | Billion-scale vectors with sub-50ms latency is non-trivial |
| Operational Complexity | Medium | Fewer moving parts than streaming systems, but memory management critical |
| Interview Frequency | Very High | Trending topic (2025+) due to LLM/RAG adoption |

---

## Key Characteristics

| Characteristic | Value | Implication |
|----------------|-------|-------------|
| **Data Type** | High-dimensional vectors (128-4096 dims) | Specialized storage and indexing required |
| **Query Pattern** | k-Nearest Neighbor (k-NN) similarity search | Approximate algorithms needed at scale |
| **Write Pattern** | Batch ingestion + real-time updates | Index rebuild vs incremental update trade-off |
| **Read:Write Ratio** | 100:1 to 1000:1 (read-heavy) | Optimize for query throughput |
| **Latency Target** | <50ms p99 for queries | In-memory indexes preferred |
| **Accuracy Metric** | Recall@k (typically >95%) | Trade-off between speed and accuracy |
| **Scale** | Millions to billions of vectors | Sharding and compression essential |

---

## Quick Navigation

| Document | Description |
|----------|-------------|
| [01 - Requirements & Estimations](./01-requirements-and-estimations.md) | Functional/non-functional requirements, capacity planning |
| [02 - High-Level Design](./02-high-level-design.md) | Architecture, components, data flow |
| [03 - Low-Level Design](./03-low-level-design.md) | Data model, APIs, algorithm pseudocode |
| [04 - Deep Dive & Bottlenecks](./04-deep-dive-and-bottlenecks.md) | HNSW internals, filtered search, real-time indexing |
| [05 - Scalability & Reliability](./05-scalability-and-reliability.md) | Sharding, replication, failure handling |
| [06 - Security & Compliance](./06-security-and-compliance.md) | Authentication, encryption, threat model |
| [07 - Observability](./07-observability.md) | Metrics, logging, tracing, alerting |
| [08 - Interview Guide](./08-interview-guide.md) | Pacing, trap questions, trade-offs |

---

## Core Algorithm Comparison

### ANN (Approximate Nearest Neighbor) Algorithms

| Algorithm | Type | Search Complexity | Memory | Build Time | Best For |
|-----------|------|-------------------|--------|------------|----------|
| **HNSW** | Graph-based | O(log n) | High (~5KB/vector) | Medium | Low-latency, high-recall |
| **IVF** | Partition-based | O(n/k × nprobe) | Low | Fast | Disk-friendly, good with PQ |
| **IVF-PQ** | Partition + Quantization | O(n/k × nprobe) | Very Low | Medium | Billion-scale, cost-sensitive |
| **DiskANN** | Graph on SSD | O(log n) | Low | Slow | Larger-than-RAM datasets |
| **ScaNN** | Quantization + Reordering | O(n/k) | Medium | Fast | Google-scale production |
| **CAGRA** | GPU-native graph | O(log n) | GPU VRAM | Very Fast | GPU-accelerated workloads |

### Algorithm Selection Guide

```
                    Latency Requirement

         <10ms                              <100ms
           │                                   │
           ▼                                   ▼
    ┌─────────────┐                    ┌─────────────┐
    │  HNSW       │                    │  IVF-PQ     │
    │  (in-memory)│                    │  (disk-ok)  │
    └─────────────┘                    └─────────────┘
           │                                   │
           │ Memory Constrained?               │ Billion-scale?
           │ Yes                               │ Yes
           ▼                                   ▼
    ┌─────────────┐                    ┌─────────────┐
    │  DiskANN    │                    │  Sharded    │
    │  (SSD)      │                    │  IVF-PQ     │
    └─────────────┘                    └─────────────┘
```

### Distance Metrics

| Metric | Formula | Use Case | Normalized Vectors? |
|--------|---------|----------|---------------------|
| **Cosine Similarity** | cos(θ) = A·B / (\|\|A\|\| × \|\|B\|\|) | Text embeddings, semantic search | Invariant to magnitude |
| **Dot Product (IP)** | A·B = Σ(aᵢ × bᵢ) | Normalized embeddings, transformers | Equivalent to cosine if normalized |
| **Euclidean (L2)** | \|\|A-B\|\| = √Σ(aᵢ-bᵢ)² | Clustering, image embeddings | Sensitive to magnitude |

**Guidance:** Match the distance metric to what your embedding model was trained with. For example:
- OpenAI `text-embedding-3-*`: Cosine similarity
- Sentence Transformers: Usually cosine similarity
- CLIP: Cosine similarity (normalized)

---

## Architecture Patterns

### Pattern 1: In-Memory HNSW (Low Latency)

```
┌─────────────────────────────────────────────────────────┐
│                    Architecture                          │
├─────────────────────────────────────────────────────────┤
│  • Full index in RAM                                     │
│  • HNSW graph for O(log n) search                       │
│  • Persistence via snapshots + WAL                      │
│  • Best: <10ms p99, <100M vectors per node              │
├─────────────────────────────────────────────────────────┤
│  Examples: Redis Vector Search, Qdrant, Pinecone        │
└─────────────────────────────────────────────────────────┘
```

### Pattern 2: Disk-Based with Quantization (Cost Optimized)

```
┌─────────────────────────────────────────────────────────┐
│                    Architecture                          │
├─────────────────────────────────────────────────────────┤
│  • Compressed vectors (PQ: 32x compression)             │
│  • IVF index on disk, centroids in memory               │
│  • Billions of vectors at low cost                      │
│  • Best: <100ms p99, 1B+ vectors                        │
├─────────────────────────────────────────────────────────┤
│  Examples: Milvus, Faiss IVF-PQ, DiskANN                │
└─────────────────────────────────────────────────────────┘
```

### Pattern 3: Hybrid Search (Vector + Keyword)

```
┌─────────────────────────────────────────────────────────┐
│                    Architecture                          │
├─────────────────────────────────────────────────────────┤
│  • Vector index (HNSW) + Inverted index (BM25)          │
│  • Fusion scoring (alpha-weighted combination)          │
│  • 42% better relevance than vector-only (RAG)          │
│  • Best: Search applications, RAG pipelines             │
├─────────────────────────────────────────────────────────┤
│  Examples: Weaviate, Vespa, Elasticsearch               │
└─────────────────────────────────────────────────────────┘
```

---

## Real-World Implementations

| System | Company | Architecture | Key Innovation | Scale |
|--------|---------|--------------|----------------|-------|
| **Pinecone** | Pinecone | Serverless, slab-based | IVF + per-cluster HNSW, auto-scaling | 1.4B vectors, 5.7K QPS |
| **Milvus** | Zilliz | Disaggregated storage/compute | Cloud-native, GPU support | Billion-scale, open-source |
| **Weaviate** | Weaviate | HNSW + inverted index | Hybrid search (vector + BM25) | 65% Fortune 500 AI teams |
| **Qdrant** | Qdrant | Rust, HNSW | SIMD acceleration, filtering | 8ms p95 @ 1M vectors |
| **Vespa** | Yahoo | Full-stack AI search | Real-time updates, 100K/sec | Perplexity, Spotify |
| **pgvector** | PostgreSQL | Extension, HNSW/IVF | SQL integration, ACID | 10-50M vectors |
| **Redis** | Redis | In-memory, HNSW | Lowest latency (<10ms) | Memory-bound |
| **Chroma** | Chroma | Embedded, lightweight | Zero-config, LangChain native | <10M vectors, prototyping |

---

## Key Trade-offs Visualization

```
                        RECALL (Accuracy)
                              ▲
                              │
                    99% ──────┼─────── HNSW (high M, ef)
                              │           │
                    95% ──────┼───────────┼──── DiskANN
                              │           │        │
                    90% ──────┼───────────┼────────┼──── IVF-PQ
                              │           │        │        │
                              └───────────┴────────┴────────┴───► LATENCY
                                  5ms      20ms    50ms   100ms

─────────────────────────────────────────────────────────────────

                          MEMORY USAGE
                              ▲
                              │
                   500GB ─────┼──── HNSW (100M vectors)
                              │
                   100GB ─────┼──── DiskANN
                              │
                    10GB ─────┼──── IVF-PQ (32x compression)
                              │
                              └────────────────────────────────►
                                        100M vectors
```

---

## When to Use a Vector Database

### Use When

- **Semantic search**: Finding documents by meaning, not keywords
- **RAG pipelines**: Retrieval for LLM context augmentation
- **Recommendation systems**: Content-based or hybrid recommendations
- **Image/video similarity**: Reverse image search, duplicate detection
- **Anomaly detection**: Finding outliers in embedding space
- **Scale**: >1M vectors with latency requirements

### Avoid When

- **Exact matching**: Use traditional databases (PostgreSQL, Elasticsearch)
- **Small scale**: <100K vectors can use brute-force
- **Structured queries only**: No vector similarity needed
- **Strong consistency required**: Most vector DBs are eventually consistent
- **Budget constrained**: In-memory HNSW is expensive at scale

---

## Interview Readiness Checklist

### Must Know

- [ ] What is ANN and why is it needed (exact k-NN is O(n))
- [ ] How HNSW works (layers, greedy search, M/ef parameters)
- [ ] How IVF works (k-means clustering, nlist/nprobe)
- [ ] Product Quantization for compression (codebooks, subvectors)
- [ ] Distance metrics and when to use each
- [ ] CAP theorem application (AP preference, eventual consistency)

### Should Know

- [ ] Memory estimation (768-dim float32 = 3KB/vector + index overhead)
- [ ] HNSW parameter tuning (M, ef_construction, ef_search)
- [ ] Filtered search approaches (pre/post/in-filter)
- [ ] Sharding strategies for billion-scale
- [ ] Real-time indexing challenges

### Nice to Know

- [ ] DiskANN for larger-than-RAM
- [ ] GPU acceleration (NVIDIA cuVS, CAGRA)
- [ ] Hybrid search fusion algorithms
- [ ] Quantization variants (scalar, binary, product)
- [ ] Multi-tenancy patterns

---

## References & Further Reading

### Papers
- Malkov & Yashunin (2018): "Efficient and Robust Approximate Nearest Neighbor using Hierarchical Navigable Small World Graphs" - HNSW
- Jégou et al. (2011): "Product Quantization for Nearest Neighbor Search" - PQ
- Subramanya et al. (2019): "DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node" - Microsoft DiskANN

### Engineering Blogs
- [Pinecone Learning Center](https://www.pinecone.io/learn/) - Comprehensive ANN tutorials
- [Milvus Blog](https://milvus.io/blog) - Distributed vector DB architecture
- [Weaviate Blog](https://weaviate.io/blog) - Hybrid search deep dives
- [Meta Engineering](https://engineering.fb.com/) - Faiss and GPU acceleration

### Benchmarks
- [ANN-Benchmarks](https://github.com/erikbern/ann-benchmarks) - Standardized algorithm comparisons
- [VectorDBBench](https://github.com/zilliztech/VectorDBBench) - Database-level benchmarks

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0 | 2026-01 | Initial release covering HNSW, IVF, PQ, major implementations |
